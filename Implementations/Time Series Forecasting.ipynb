{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    " \n",
    "    \n",
    "sys.path.insert(1, '../../tsad/')\n",
    "from tsad.useful.ts import ts_train_test_split     \n",
    "from tsad.models.fit import fit, set_determenistic\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import numpy as np\n",
    "from tsad.models.lstm import SimpleLSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tsad.useful.iterators import Loader\n",
    "    \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63c2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b3ead9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'skab.zip'\n",
    "if not os.path.exists('skab.zip'):\n",
    "    import requests\n",
    "    url = 'https://github.com/waico/SKAB/archive/refs/heads/master.zip'\n",
    "    r = requests.get(url)\n",
    "    open(fname , 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d547419",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(fname)\n",
    "train_datasets = []\n",
    "test_datasets  = []\n",
    "for file  in z.namelist():\n",
    "    if '.csv' in file:\n",
    "        if 'anomaly-free' in file:\n",
    "            test_datasets.append(pd.read_csv(z.open(file),index_col='datetime',sep=';',parse_dates=True))\n",
    "        else:\n",
    "            train_datasets.append(pd.read_csv(z.open(file),index_col='datetime',sep=';',parse_dates=True).drop(columns= ['anomaly','changepoint']))\n",
    "\n",
    "scaler = StandardScaler().fit(pd.concat(train_datasets+test_datasets))\n",
    "train_datasets = [pd.DataFrame(scaler.transform(df),index=df.index,columns=df.columns) for df in train_datasets]\n",
    "test_datasets = [pd.DataFrame(scaler.transform(df),index=df.index,columns=df.columns) for df in test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46baf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fc5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = []\n",
    "y_train_ts = []\n",
    "for df in train_datasets:\n",
    "    sets = ts_train_test_split(df,30,test_size=0)\n",
    "    X_train_ts+=sets[0]\n",
    "    y_train_ts+=sets[2]\n",
    "\n",
    "sets = ts_train_test_split(test_datasets[0],30,test_size=0)\n",
    "X_test_ts=sets[0]\n",
    "y_test_ts=sets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b026e",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cf0a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(x):\n",
    "    return x.median()\n",
    "\n",
    "def last_value(x):\n",
    "    return x.iloc[-1]\n",
    "\n",
    "def first_value(x):\n",
    "    return x.iloc[0]\n",
    "\n",
    "def q025(x):\n",
    "    return x.apply(lambda var: np.quantile(var,0.25))\n",
    "\n",
    "def q075(x):\n",
    "    return x.apply(lambda var: np.quantile(var,0.75))\n",
    "\n",
    "def diff_mean(x):\n",
    "    return x.diff().mean()\n",
    "\n",
    "def diff_min(x):\n",
    "    return x.diff().min()\n",
    "\n",
    "def diff_max(x):\n",
    "    return x.diff().max()\n",
    "\n",
    "def diff_sum(x):\n",
    "    return x.diff().sum()\n",
    "\n",
    "\n",
    "def lin_koef(x):\n",
    "    all_df = []\n",
    "    for col in x:\n",
    "        ms = x.reset_index()[col]\n",
    "        reg = LinearRegression().fit(np.array(ms.index).reshape(-1, 1),\n",
    "                                     ms.values.reshape(-1, 1))\n",
    "        all_df.append(pd.Series([reg.coef_[0][0],reg.intercept_[0]],index=['a_koef'+'-'+col,'b_koef'+'-'+col ]))\n",
    "    return pd.concat(all_df)\n",
    "\n",
    "def get_fft_features_from_df(x):\n",
    "    return pd.concat([pd.Series(x.apply(lambda ts: np.real(np.fft.fft(ts)[:5])).values.ravel(),\n",
    "                                index = ['fft_real'+str(i) for i in range(x.shape[1]*5)]),\n",
    "                      pd.Series(x.apply(lambda ts: np.imag(np.fft.fft(ts)[:5])).values.ravel(),\n",
    "                                index = ['fft_img'+str(i) for i in range(x.shape[1]*5)])])\n",
    "\n",
    "functions = [np.mean,\n",
    "             np.max,\n",
    "             np.min,\n",
    "             median,\n",
    "             np.sum, \n",
    "             q025,\n",
    "             q075,\n",
    "             last_value, \n",
    "             first_value,\n",
    "             diff_mean,\n",
    "             diff_min,\n",
    "             diff_max,\n",
    "             diff_sum,\n",
    "             lin_koef,\n",
    "             get_fft_features_from_df,\n",
    "            ] \n",
    "\n",
    "def generate_feature(batch,portion=1):\n",
    "    new_batch = []\n",
    "    for mini_df in batch:\n",
    "        new_raw = []\n",
    "        for function in functions:\n",
    "            new_raw.append(function(mini_df[-int(len(df)*portion):]).add_prefix(str(function).split(' ')[1]+'-'))\n",
    "        new_raw = pd.concat(new_raw).to_frame().T\n",
    "        new_batch.append(new_raw)\n",
    "    new_batch = pd.concat(new_batch)\n",
    "    return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6061452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([generate_feature(X_train_ts,portion=1),\n",
    "                 generate_feature(X_train_ts,portion=1/7).add_prefix('22_'),\n",
    "                 generate_feature(X_train_ts,portion=1/7/6).add_prefix('33_')],1)\n",
    "X_test = pd.concat([generate_feature(X_test_ts,portion=1),\n",
    "                     generate_feature(X_test_ts,portion=1/7).add_prefix('22_'),\n",
    "                     generate_feature(X_test_ts,portion=1/7/6).add_prefix('33_')],1)\n",
    "y_train = np.concatenate([df.values for df in y_train_ts])\n",
    "y_test  = np.concatenate([df.values for df in y_test_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "919b6c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.39\n"
     ]
    }
   ],
   "source": [
    "reg = CatBoostRegressor(loss_function='MultiRMSE', verbose=False)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "RMSE = mse(y_pred,y_test,squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c8911",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "718a4696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.47\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for col in df:\n",
    "    for i in tqdm(range(window,len(df))):\n",
    "        try:\n",
    "            ts = df[col][:i]\n",
    "            model = ARIMA(ts[:i], order=(30,1,0))\n",
    "            res = model.fit()\n",
    "            preds.append(res.forecast(1).values)\n",
    "            trues.append(df[col][i:i+1].values)\n",
    "        except:\n",
    "            pass\n",
    "RMSE = mse(np.array(preds),np.array(trues),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d0061",
   "metadata": {},
   "source": [
    "# DMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e106be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMLP(nn.Module):\n",
    "    def __init__(self, in_features, n_hidden, n_output, seed=None):\n",
    "        set_determenistic(seed)\n",
    "        super(DMLP, self).__init__()\n",
    "        middle1 = int(1/2*(n_hidden+in_features))\n",
    "        middle2 = int(1/2*(n_hidden+n_output))\n",
    "        \n",
    "        self.sig = torch.nn.functional.sigmoid\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features=in_features, out_features=middle1 )\n",
    "        self.linear2 = nn.Linear(in_features=middle1, out_features=n_hidden)   \n",
    "        self.linear3 = nn.Linear(in_features=n_hidden, out_features=middle2)   \n",
    "        self.linear4 = nn.Linear(in_features=middle2, out_features=n_output)   \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sig(self.linear1(x))\n",
    "        x = self.sig(self.linear2(x))\n",
    "        x = self.sig(self.linear3(x))\n",
    "        out = self.linear4(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def run_epoch(self, iterator, optimizer, criterion, points_ahead=1, phase='train', device=torch.device('cuda:0'), encod_decode_model=False):\n",
    "        self.to(device)\n",
    "        \n",
    "        is_train = (phase == 'train')\n",
    "        if is_train:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "\n",
    "        all_y_preds = []\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            for i, (x,y) in enumerate(iterator):\n",
    "                x,y = np.array(x),np.array(y) #df.index rif of\n",
    "                \n",
    "                x = torch.tensor(x).float().to(device).requires_grad_().squeeze()\n",
    "                y_true = torch.tensor(y).float().to(device)\n",
    "                y_pred = self.forward(x).unsqueeze(1)                \n",
    "\n",
    "                if phase == 'forecast':\n",
    "                    all_y_preds.append(y_pred)\n",
    "                    continue # in case of pahse = 'forecast' criterion is None\n",
    "                        \n",
    "                loss = criterion(y_pred,y_true)\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "        if phase != 'forecast':\n",
    "            return epoch_loss / len(iterator)#, n_true_predicted / n_predicted\n",
    "        else:\n",
    "            return torch.cat(all_y_preds).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82e8e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_test_split = ([df.values.ravel().reshape(1,-1) for df in X_train_ts],\n",
    "                        [df.values.ravel().reshape(1,-1) for df in X_test_ts],\n",
    "                        [df.values for df in y_train_ts],\n",
    "                        [df.values for df in y_test_ts]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9a70589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net paramaeters: 31028\n",
      "Samples: 36381\n"
     ]
    }
   ],
   "source": [
    "inp_size = res_train_test_split[0][0].shape[1]\n",
    "out_size = res_train_test_split[-1][0].shape[1]\n",
    "model = DMLP(inp_size,out_size,out_size)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Net paramaeters: {pytorch_total_params}')\n",
    "print(f'Samples: {len(res_train_test_split[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e1600ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \n",
      "\tTrain Loss: 0.261 \n",
      "\t Val. Loss: 0.346 \n",
      "\n",
      "Epoch: 99 \n",
      "\tTrain Loss: 0.261 \n",
      "\t Val. Loss: 0.343 \n",
      "\n",
      "Epoch: 98 \n",
      "\tTrain Loss: 0.262 \n",
      "\t Val. Loss: 0.340 \n",
      "\n",
      "Epoch: 97 \n",
      "\tTrain Loss: 0.262 \n",
      "\t Val. Loss: 0.338 \n",
      "\n",
      "Epoch: 96 \n",
      "\tTrain Loss: 0.262 \n",
      "\t Val. Loss: 0.336 \n",
      "\n",
      "Epoch: 95 \n",
      "\tTrain Loss: 0.263 \n",
      "\t Val. Loss: 0.333 \n",
      "\n",
      "Epoch: 94 \n",
      "\tTrain Loss: 0.263 \n",
      "\t Val. Loss: 0.331 \n",
      "\n",
      "Epoch: 93 \n",
      "\tTrain Loss: 0.263 \n",
      "\t Val. Loss: 0.329 \n",
      "\n",
      "Epoch: 92 \n",
      "\tTrain Loss: 0.264 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 91 \n",
      "\tTrain Loss: 0.264 \n",
      "\t Val. Loss: 0.325 \n",
      "\n",
      "Epoch: 90 \n",
      "\tTrain Loss: 0.264 \n",
      "\t Val. Loss: 0.324 \n",
      "\n",
      "Epoch: 89 \n",
      "\tTrain Loss: 0.265 \n",
      "\t Val. Loss: 0.322 \n",
      "\n",
      "Epoch: 88 \n",
      "\tTrain Loss: 0.265 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 87 \n",
      "\tTrain Loss: 0.265 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 86 \n",
      "\tTrain Loss: 0.266 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 85 \n",
      "\tTrain Loss: 0.266 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 84 \n",
      "\tTrain Loss: 0.266 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 83 \n",
      "\tTrain Loss: 0.266 \n",
      "\t Val. Loss: 0.315 \n",
      "\n",
      "Epoch: 82 \n",
      "\tTrain Loss: 0.267 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 81 \n",
      "\tTrain Loss: 0.267 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 80 \n",
      "\tTrain Loss: 0.267 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 79 \n",
      "\tTrain Loss: 0.267 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 78 \n",
      "\tTrain Loss: 0.268 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 77 \n",
      "\tTrain Loss: 0.268 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 76 \n",
      "\tTrain Loss: 0.268 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 75 \n",
      "\tTrain Loss: 0.268 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 74 \n",
      "\tTrain Loss: 0.268 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 73 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 72 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 71 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 70 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 69 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 68 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.312 \n",
      "\n",
      "Epoch: 67 \n",
      "\tTrain Loss: 0.269 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 66 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 65 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 64 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 63 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 62 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 61 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.315 \n",
      "\n",
      "Epoch: 60 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.315 \n",
      "\n",
      "Epoch: 59 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 58 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 57 \n",
      "\tTrain Loss: 0.270 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 56 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 55 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 54 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 53 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 52 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 51 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 50 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 49 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 48 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 47 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 46 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 45 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 44 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 43 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 42 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 41 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 40 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 39 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.322 \n",
      "\n",
      "Epoch: 38 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.323 \n",
      "\n",
      "Epoch: 37 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.323 \n",
      "\n",
      "Epoch: 36 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.323 \n",
      "\n",
      "Epoch: 35 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.322 \n",
      "\n",
      "Epoch: 34 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 33 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 32 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 31 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 30 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 29 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 28 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 27 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 26 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 25 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 24 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 23 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 22 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 21 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 20 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 19 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.319 \n",
      "\n",
      "Epoch: 18 \n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 17 \n",
      "\tTrain Loss: 0.280 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 16 \n",
      "\tTrain Loss: 0.281 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 15 \n",
      "\tTrain Loss: 0.282 \n",
      "\t Val. Loss: 0.315 \n",
      "\n",
      "Epoch: 14 \n",
      "\tTrain Loss: 0.283 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 13 \n",
      "\tTrain Loss: 0.284 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 12 \n",
      "\tTrain Loss: 0.286 \n",
      "\t Val. Loss: 0.322 \n",
      "\n",
      "Epoch: 11 \n",
      "\tTrain Loss: 0.288 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 10 \n",
      "\tTrain Loss: 0.292 \n",
      "\t Val. Loss: 0.333 \n",
      "\n",
      "Epoch: 09 \n",
      "\tTrain Loss: 0.299 \n",
      "\t Val. Loss: 0.337 \n",
      "\n",
      "Epoch: 08 \n",
      "\tTrain Loss: 0.309 \n",
      "\t Val. Loss: 0.350 \n",
      "\n",
      "Epoch: 07 \n",
      "\tTrain Loss: 0.319 \n",
      "\t Val. Loss: 0.350 \n",
      "\n",
      "Epoch: 06 \n",
      "\tTrain Loss: 0.330 \n",
      "\t Val. Loss: 0.341 \n",
      "\n",
      "Epoch: 05 \n",
      "\tTrain Loss: 0.346 \n",
      "\t Val. Loss: 0.342 \n",
      "\n",
      "Epoch: 04 \n",
      "\tTrain Loss: 0.379 \n",
      "\t Val. Loss: 0.354 \n",
      "\n",
      "Epoch: 03 \n",
      "\tTrain Loss: 0.411 \n",
      "\t Val. Loss: 0.393 \n",
      "\n",
      "Epoch: 02 \n",
      "\tTrain Loss: 0.437 \n",
      "\t Val. Loss: 0.384 \n",
      "\n",
      "Epoch: 01 \n",
      "\tTrain Loss: 0.756 \n",
      "\t Val. Loss: 0.400 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3111343335460972"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "fit(model,\n",
    "    optimiser=optimizer,\n",
    "    criterion=criterion,\n",
    "    res_train_test_split=res_train_test_split,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    best_model_file='./best_ae.pth',\n",
    "    points_ahead=1,\n",
    "    random_state=777,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2a5038a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.47\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_pred_test =  model.run_epoch(Loader(res_train_test_split[1],res_train_test_split[0],32), optimizer=None, criterion=None, phase='forecast').squeeze(1)\n",
    "RMSE = mse(y_pred_test,np.squeeze(res_train_test_split[-1],1),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc688dd",
   "metadata": {},
   "source": [
    "# AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52523164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net paramaeters: 29980\n",
      "Samples: 36381\n"
     ]
    }
   ],
   "source": [
    "inp_size = res_train_test_split[0][0].shape[1]\n",
    "out_size = res_train_test_split[-1][0].shape[1]\n",
    "hidden_size= int(res_train_test_split[-1][0].shape[1]/2)\n",
    "\n",
    "model = DMLP(inp_size,hidden_size,out_size)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Net paramaeters: {pytorch_total_params}')\n",
    "print(f'Samples: {len(res_train_test_split[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a402d65e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.368 \n",
      "\n",
      "Epoch: 99 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.367 \n",
      "\n",
      "Epoch: 98 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.367 \n",
      "\n",
      "Epoch: 97 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.366 \n",
      "\n",
      "Epoch: 96 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.366 \n",
      "\n",
      "Epoch: 95 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.365 \n",
      "\n",
      "Epoch: 94 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.364 \n",
      "\n",
      "Epoch: 93 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.364 \n",
      "\n",
      "Epoch: 92 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.363 \n",
      "\n",
      "Epoch: 91 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.362 \n",
      "\n",
      "Epoch: 90 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.361 \n",
      "\n",
      "Epoch: 89 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.360 \n",
      "\n",
      "Epoch: 88 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.359 \n",
      "\n",
      "Epoch: 87 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.358 \n",
      "\n",
      "Epoch: 86 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.358 \n",
      "\n",
      "Epoch: 85 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.357 \n",
      "\n",
      "Epoch: 84 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.356 \n",
      "\n",
      "Epoch: 83 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.355 \n",
      "\n",
      "Epoch: 82 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.354 \n",
      "\n",
      "Epoch: 81 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.353 \n",
      "\n",
      "Epoch: 80 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.352 \n",
      "\n",
      "Epoch: 79 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.351 \n",
      "\n",
      "Epoch: 78 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.349 \n",
      "\n",
      "Epoch: 77 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.348 \n",
      "\n",
      "Epoch: 76 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.347 \n",
      "\n",
      "Epoch: 75 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.346 \n",
      "\n",
      "Epoch: 74 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.345 \n",
      "\n",
      "Epoch: 73 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.344 \n",
      "\n",
      "Epoch: 72 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.343 \n",
      "\n",
      "Epoch: 71 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.342 \n",
      "\n",
      "Epoch: 70 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.341 \n",
      "\n",
      "Epoch: 69 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.340 \n",
      "\n",
      "Epoch: 68 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.339 \n",
      "\n",
      "Epoch: 67 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.338 \n",
      "\n",
      "Epoch: 66 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.337 \n",
      "\n",
      "Epoch: 65 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.336 \n",
      "\n",
      "Epoch: 64 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.335 \n",
      "\n",
      "Epoch: 63 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.334 \n",
      "\n",
      "Epoch: 62 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.334 \n",
      "\n",
      "Epoch: 61 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.333 \n",
      "\n",
      "Epoch: 60 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.332 \n",
      "\n",
      "Epoch: 59 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.331 \n",
      "\n",
      "Epoch: 58 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.331 \n",
      "\n",
      "Epoch: 57 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.330 \n",
      "\n",
      "Epoch: 56 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.330 \n",
      "\n",
      "Epoch: 55 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.329 \n",
      "\n",
      "Epoch: 54 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.329 \n",
      "\n",
      "Epoch: 53 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.328 \n",
      "\n",
      "Epoch: 52 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.328 \n",
      "\n",
      "Epoch: 51 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 50 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 49 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.326 \n",
      "\n",
      "Epoch: 48 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.326 \n",
      "\n",
      "Epoch: 47 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.325 \n",
      "\n",
      "Epoch: 46 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.324 \n",
      "\n",
      "Epoch: 45 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.323 \n",
      "\n",
      "Epoch: 44 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.321 \n",
      "\n",
      "Epoch: 43 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.320 \n",
      "\n",
      "Epoch: 42 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.318 \n",
      "\n",
      "Epoch: 41 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 40 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.316 \n",
      "\n",
      "Epoch: 39 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 38 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 37 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.313 \n",
      "\n",
      "Epoch: 36 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 35 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.310 \n",
      "\n",
      "Epoch: 34 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.310 \n",
      "\n",
      "Epoch: 33 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.309 \n",
      "\n",
      "Epoch: 32 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.308 \n",
      "\n",
      "Epoch: 31 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.308 \n",
      "\n",
      "Epoch: 30 \n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 0.307 \n",
      "\n",
      "Epoch: 29 \n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 0.307 \n",
      "\n",
      "Epoch: 28 \n",
      "\tTrain Loss: 0.280 \n",
      "\t Val. Loss: 0.307 \n",
      "\n",
      "Epoch: 27 \n",
      "\tTrain Loss: 0.281 \n",
      "\t Val. Loss: 0.306 \n",
      "\n",
      "Epoch: 26 \n",
      "\tTrain Loss: 0.281 \n",
      "\t Val. Loss: 0.306 \n",
      "\n",
      "Epoch: 25 \n",
      "\tTrain Loss: 0.282 \n",
      "\t Val. Loss: 0.307 \n",
      "\n",
      "Epoch: 24 \n",
      "\tTrain Loss: 0.283 \n",
      "\t Val. Loss: 0.308 \n",
      "\n",
      "Epoch: 23 \n",
      "\tTrain Loss: 0.283 \n",
      "\t Val. Loss: 0.309 \n",
      "\n",
      "Epoch: 22 \n",
      "\tTrain Loss: 0.284 \n",
      "\t Val. Loss: 0.310 \n",
      "\n",
      "Epoch: 21 \n",
      "\tTrain Loss: 0.284 \n",
      "\t Val. Loss: 0.311 \n",
      "\n",
      "Epoch: 20 \n",
      "\tTrain Loss: 0.285 \n",
      "\t Val. Loss: 0.314 \n",
      "\n",
      "Epoch: 19 \n",
      "\tTrain Loss: 0.286 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 18 \n",
      "\tTrain Loss: 0.287 \n",
      "\t Val. Loss: 0.322 \n",
      "\n",
      "Epoch: 17 \n",
      "\tTrain Loss: 0.289 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 16 \n",
      "\tTrain Loss: 0.291 \n",
      "\t Val. Loss: 0.333 \n",
      "\n",
      "Epoch: 15 \n",
      "\tTrain Loss: 0.295 \n",
      "\t Val. Loss: 0.340 \n",
      "\n",
      "Epoch: 14 \n",
      "\tTrain Loss: 0.302 \n",
      "\t Val. Loss: 0.340 \n",
      "\n",
      "Epoch: 13 \n",
      "\tTrain Loss: 0.309 \n",
      "\t Val. Loss: 0.350 \n",
      "\n",
      "Epoch: 12 \n",
      "\tTrain Loss: 0.317 \n",
      "\t Val. Loss: 0.355 \n",
      "\n",
      "Epoch: 11 \n",
      "\tTrain Loss: 0.325 \n",
      "\t Val. Loss: 0.360 \n",
      "\n",
      "Epoch: 10 \n",
      "\tTrain Loss: 0.333 \n",
      "\t Val. Loss: 0.357 \n",
      "\n",
      "Epoch: 09 \n",
      "\tTrain Loss: 0.344 \n",
      "\t Val. Loss: 0.358 \n",
      "\n",
      "Epoch: 08 \n",
      "\tTrain Loss: 0.357 \n",
      "\t Val. Loss: 0.362 \n",
      "\n",
      "Epoch: 07 \n",
      "\tTrain Loss: 0.373 \n",
      "\t Val. Loss: 0.365 \n",
      "\n",
      "Epoch: 06 \n",
      "\tTrain Loss: 0.390 \n",
      "\t Val. Loss: 0.367 \n",
      "\n",
      "Epoch: 05 \n",
      "\tTrain Loss: 0.409 \n",
      "\t Val. Loss: 0.357 \n",
      "\n",
      "Epoch: 04 \n",
      "\tTrain Loss: 0.425 \n",
      "\t Val. Loss: 0.396 \n",
      "\n",
      "Epoch: 03 \n",
      "\tTrain Loss: 0.461 \n",
      "\t Val. Loss: 0.394 \n",
      "\n",
      "Epoch: 02 \n",
      "\tTrain Loss: 0.648 \n",
      "\t Val. Loss: 0.440 \n",
      "\n",
      "Epoch: 01 \n",
      "\tTrain Loss: 1.003 \n",
      "\t Val. Loss: 0.846 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30637951537563995"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "fit(model,\n",
    "    optimiser=optimizer,\n",
    "    criterion=criterion,\n",
    "    res_train_test_split=res_train_test_split,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    best_model_file='./best_ae.pth',\n",
    "    points_ahead=1,\n",
    "    random_state=777,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c22be956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.45\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_pred_test =  model.run_epoch(Loader(res_train_test_split[1],res_train_test_split[0],32), optimizer=None, criterion=None, phase='forecast').squeeze(1)\n",
    "RMSE = mse(y_pred_test,np.squeeze(res_train_test_split[-1],1),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5bbc0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cff0702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):  \n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        set_determenistic(seed)\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=7, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(182, 8)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def run_epoch(self, iterator, optimizer, criterion, points_ahead=1, phase='train', device=torch.device('cuda:0'), encod_decode_model=False):\n",
    "        self.to(device)\n",
    "        \n",
    "        is_train = (phase == 'train')\n",
    "        if is_train:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "\n",
    "        all_y_preds = []\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            for i, (x,y) in enumerate(iterator):\n",
    "                x,y = np.array(x),np.array(y) #df.index rif of\n",
    "                \n",
    "                x = torch.tensor(x).float().to(device).requires_grad_().squeeze()\n",
    "                y_true = torch.tensor(y).float().to(device)\n",
    "                y_pred = self.forward(x).unsqueeze(1)                \n",
    "\n",
    "                if phase == 'forecast':\n",
    "                    all_y_preds.append(y_pred)\n",
    "                    continue # in case of pahse = 'forecast' criterion is None\n",
    "                        \n",
    "                loss = criterion(y_pred,y_true)\n",
    "                if is_train:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "        if phase != 'forecast':\n",
    "            return epoch_loss / len(iterator)#, n_true_predicted / n_predicted\n",
    "        else:\n",
    "            return torch.cat(all_y_preds).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49e26699",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_test_split = ([df.values for df in X_train_ts],\n",
    "                        [df.values for df in X_test_ts],\n",
    "                        [df.values for df in y_train_ts],\n",
    "                        [df.values for df in y_test_ts]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "23b3f334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net paramaeters: 1690\n",
      "Samples: 36381\n"
     ]
    }
   ],
   "source": [
    "inp_size = res_train_test_split[0][0].shape[1]\n",
    "out_size = res_train_test_split[-1][0].shape[1]\n",
    "model = CNN()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Net paramaeters: {pytorch_total_params}')\n",
    "print(f'Samples: {len(res_train_test_split[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ded518c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAstElEQVR4nO3de5hddX3v8fd37b1nJjOTe4YAmYQEiISA3M6cqIAYtLagaEDbQqpWiy0HlaNWrULPo9bS81Sex9Nj7dFyUqTaesnhqNQc5VKkpmgpkIFGJIRLCAEmhFwGksllLvvyPX/81p69ZrInk0lmzU5mf17Ps5+91m9d9nfNZX3377d+a/3M3RERERkuqnUAIiJybFKCEBGRqpQgRESkKiUIERGpSglCRESqytY6gPE0Z84cX7hwYa3DEBE5bjz66KO73L2t2rJJlSAWLlxIZ2dnrcMQETlumNkLIy1TE5OIiFSlBCEiIlUpQYiISFWT6hqEiMhY5PN5urq66Ovrq3UoqWtqaqK9vZ1cLnfY2yhBiEjd6urqYurUqSxcuBAzq3U4qXF3uru76erqYtGiRYe9nZqYRKRu9fX1MXv27EmdHADMjNmzZ4+5ppRqgjCzy8zsaTPbZGY3Vlm+wsweN7P1ZtZpZhcnlm0xs1+Xl6UZp4jUr8meHMqO5DhTa2IyswzwdeDtQBewzszWuPuTidXuB9a4u5vZOcAdwJLE8kvdfVdaMZZ97f5nOXf+DN7yuqr3ioiI1KU0axDLgE3uvtndB4DVwIrkCu6+zysDUrQANRmc4m/XPscvn91Zi48WkTrW3d3Neeedx3nnnceJJ57IvHnzBucHBgYOuW1nZycf//jHU40vzYvU84CXEvNdwBuGr2RmVwF/CZwAvDOxyIF/NjMH/re7r6r2IWZ2HXAdwIIFC44o0GzGyBc1cJKITKzZs2ezfv16AP7sz/6M1tZWPvOZzwwuLxQKZLPVT9MdHR10dHSkGl+aNYhqDV4HnYXd/U53XwJcCdycWHSRu18AXA58zMwuqfYh7r7K3TvcvaOt7ciaiHKZiEKpdETbioiMpw996EN86lOf4tJLL+Vzn/scjzzyCBdeeCHnn38+F154IU8//TQAa9eu5YorrgBCcrn22mtZvnw5p556Kl/72tfGJZY0axBdwPzEfDvw8kgru/sDZnaamc1x913u/nJcvsPM7iQ0WT2QRqDZyCioBiFS1770/zbw5Ms947rPpSdP44vvOmvM2z3zzDP87Gc/I5PJ0NPTwwMPPEA2m+VnP/sZf/qnf8oPf/jDg7Z56qmn+PnPf87evXs544wz+MhHPjKmex6qSTNBrAMWm9kiYCtwDfB7yRXM7HTgufgi9QVAA9BtZi1A5O574+nfBP48rUBzmUhNTCJyzPid3/kdMpkMAHv27OGDH/wgzz77LGZGPp+vus073/lOGhsbaWxs5IQTTmD79u20t7cfVRypJQh3L5jZDcC9QAa43d03mNn18fJbgfcCv29meaAXuDpOFnOBO+NuWVnge+5+T1qxZjOmJiaROnck3/TT0tLSMjj9+c9/nksvvZQ777yTLVu2sHz58qrbNDY2Dk5nMhkKhcJRx5HqndTufhdw17CyWxPTtwC3VNluM3BumrElqYlJRI5Ve/bsYd68eQB861vfmtDP1p3UhCamgaJqECJy7PnsZz/LTTfdxEUXXUSxWJzQz7bKbQjHv46ODj+SAYOu+Jtf0NbayN//wbIUohKRY9XGjRs588wzax3GhKl2vGb2qLtX7S+rGgTlbq6TJ1GKiIwHJQggF0Xk1cQkIjKEEgRxLyZdpBYRGUIJAshmIvJqYhIRGUIJAshFRkFNTCIiQyhBoCYmEZFqlCAoNzGpBiEiE2v58uXce++9Q8q++tWv8tGPfnTE9Y+kK/+RUoKg3MSkGoSITKyVK1eyevXqIWWrV69m5cqVNYpoKCUIQg1C1yBEZKL99m//Nj/5yU/o7+8HYMuWLbz88st873vfo6Ojg7POOosvfvGLNYsv1WcxHS9yGVMvJpF6d/eN8Mqvx3efJ74eLv/yiItnz57NsmXLuOeee1ixYgWrV6/m6quv5qabbmLWrFkUi0Xe9ra38fjjj3POOeeMb2yHQTUIIBupBiEitZFsZio3L91xxx1ccMEFnH/++WzYsIEnn3yyJrGpBoF6MYkIh/ymn6Yrr7yST33qUzz22GP09vYyc+ZMvvKVr7Bu3TpmzpzJhz70Ifr6+moSm2oQxAMGqReTiNRAa2sry5cv59prr2XlypX09PTQ0tLC9OnT2b59O3fffXfNYlMNAo0HISK1tXLlSt7znvewevVqlixZwvnnn89ZZ53FqaeeykUXXVSzuFJNEGZ2GfDXhBHlbnP3Lw9bvgK4GSgBBeCT7v7LxPIM0Alsdfcr0oozGz/N1d2JR7ETEZkwV111FcmhF0YaGGjt2rUTE1AstSam+OT+deByYCmw0syWDlvtfuBcdz8PuBa4bdjyTwAb04qxLBeFpKBxqUVEKtK8BrEM2OTum919AFgNrEiu4O77vJI2W4DBM7SZtQPv5OCkMe6ymfBj0LjUIiIVaSaIecBLifmuuGwIM7vKzJ4CfkqoRZR9FfgsoflpRGZ2nZl1mlnnzp07jyjQXEY1CJF6NZlG1TyUIznONBNEtcb8gyJ09zvdfQlwJeF6BGZ2BbDD3R8d7UPcfZW7d7h7R1tb2xEFmivXIHQvhEhdaWpqoru7e9InCXenu7ubpqamMW2X5kXqLmB+Yr4deHmkld39ATM7zczmABcB7zazdwBNwDQz+467vz+NQLNxDULDjorUl/b2drq6ujjS1ofjSVNTE+3t7WPaJs0EsQ5YbGaLgK3ANcDvJVcws9OB59zdzewCoAHodvebgJvidZYDn0krOUAYchTQsKMidSaXy7Fo0aJah3HMSi1BuHvBzG4A7iV0c73d3TeY2fXx8luB9wK/b2Z5oBe42mtQ1xusQegahIjIoFTvg3D3u4C7hpXdmpi+BbhllH2sBdamEN4g9WISETmYHrWB7oMQEalGCYJEDUIJQkRkkBIElWsQemCfiEiFEgSVXkyqQYiIVChBkOzFpBqEiEiZEgSJR23oRjkRkUFKEIQhR0E1CBGRJCUIEhepdQ1CRGSQEgSVh/XpURsiIhVKEIQhR0F3UouIJClBkKxBqIlJRKRMCQI9rE9EpBolCBIDBqmJSURkkBIEyfEgVIMQESlTgkB3UouIVKMEgYYcFRGpJtUEYWaXmdnTZrbJzG6ssnyFmT1uZuvNrNPMLo7Lm8zsETP7lZltMLMvpRmnhhwVETlYaiPKmVkG+DrwdqALWGdma9z9ycRq9wNr4jGpzwHuAJYA/cBb3X2fmeWAX5rZ3e7+UBqxRpERmXoxiYgkpVmDWAZscvfN7j4ArAZWJFdw932JMahbAI/L3d33xeW5+JXq2TubiTQehIhIQpoJYh7wUmK+Ky4bwsyuMrOngJ8C1ybKM2a2HtgB3OfuD1f7EDO7Lm6e6ty5c+cRB5uLTDUIEZGENBOEVSk76Azs7ne6+xLgSuDmRHnR3c8D2oFlZnZ2tQ9x91Xu3uHuHW1tbUccbDYTqReTiEhCmgmiC5ifmG8HXh5pZXd/ADjNzOYMK98NrAUuG/8QK3IZ03gQIiIJaSaIdcBiM1tkZg3ANcCa5ApmdrqZWTx9AdAAdJtZm5nNiMunAL8BPJVirGQj1SBERJJS68Xk7gUzuwG4F8gAt7v7BjO7Pl5+K/Be4PfNLA/0AlfHPZpOAr4d94SKgDvc/SdpxQrhXghdgxARqUgtQQC4+13AXcPKbk1M3wLcUmW7x4Hz04xtuFwmUhOTiEiC7qQGKJU4xbdx7mv3wX1fgJceqXVEIiI1l2oN4rhQGID/8Tq+1fsa7Cd0qn1tC8z/hxoHJiJSW0oQ2QZ4w/V8rfMAO1qX8hdN34E9XbWOSkSk5tTEBLD8RtY2/xbPZxfCzIWw+6XRthARmfSUIGLZTBTGg5ixAPbvgHxfrUMSEakpJYhYQ/lO6untoaBna20DEhGpMSWIWDZjYTyI6fHN33vUzCQi9U0JIpaN4iamcg1C1yFEpM4pQcRyGQtNTNPmAaaeTCJS95QgYtlMFJqYsg0w9UQ1MYlI3VOCiOUiqww5On0+7H6xtgGJiNSYEkRsyMP6preriUlE6p4SRCw0McU1iBnzQzdXDUEqInVMCSIWmpjKNYj5UBwIN8yJiNQpJYjYkCFHy/dCqKuriNQxJYhYNjnk6AzdLCcikmqCMLPLzOxpM9tkZjdWWb7CzB43s/Vm1mlmF8fl883s52a20cw2mNkn0owTIJcccrR8s5wShIjUsdQe9x0PF/p14O1AF7DOzNa4+5OJ1e4H1sTDjJ4D3AEsAQrAp939MTObCjxqZvcN23ZcZTNGyaFYcjJN06FxunoyiUhdS7MGsQzY5O6b3X0AWA2sSK7g7vvcvTzOZwvgcfk2d38snt4LbATmpRgruUz4UeSTtQhdgxCROpZmgpgHJM+wXVQ5yZvZVWb2FPBT4NoqyxcSxqd+uNqHmNl1cfNU586dO4842GxkAOFuagjXIdTEJCJ1LM0EYVXK/KAC9zvdfQlwJXDzkB2YtQI/BD7p7j3VPsTdV7l7h7t3tLW1HXGw2bgGMeQ6hBKEiNSxNBNEFzA/Md8OvDzSyu7+AHCamc0BMLMcITl8191/lGKcQHhYHzD0Xoi+PdBXNS+JiEx6aSaIdcBiM1tkZg3ANcCa5ApmdrqZWTx9AdAAdMdl3wQ2uvtfpRjjoPI1iCF3U4MuVItI3UotQbh7AbgBuJdwkfkOd99gZteb2fXxau8FnjCz9YQeT1fHF60vAj4AvDXuArvezN6RVqyQuAaRrEGAmplEpG6l1s0VwN3vAu4aVnZrYvoW4JYq2/2S6tcwUnNwLyYlCBGpb7qTOpbNDOvF1DoXopy6uopI3VKCiGWjYTWIKILp81SDEJG6pQQRK/diGrwGATCtHXpG7HglIjKpKUHEssN7MQFMO0kJQkTqlhJELBcNuw8CYNrJsHebBg4SkbqkBBGr3EmdSBBTTw4DBx3orlFUIiK1owQRK/diyg9pYjo5vO9VM5OI1B8liFguqlKDmBY/W1DXIUSkDilBxAZrEMVhF6lBCUJE6tIhE4SZvT8xfdGwZTekFVQt5KoliNa5YBklCBGpS6PVID6VmP6bYcsOGrvheJat1sQUZUKS2LutRlGJiNTOaAnCRpiuNn9cqzxqY1iX1mknQ8/WGkQkIlJboyUIH2G62vxxrfKwvmGHNe0k6FENQkTqz2hPc11iZo8TagunxdPE86emGtkEqzzue3gNYh48t3biAxIRqbHREsSZExLFMaDyqI1hNYipJ8HA3jCyXNO0GkQmIlIbh0wQ7v5Cct7MZgOXAC+6+6NpBjbRGkZsYorvhdi7TQlCROrKaN1cf2JmZ8fTJwFPEHov/aOZfXK0nZvZZWb2tJltMrMbqyxfYWaPxyPGdZrZxYllt5vZDjN7YqwHdSQGL1If1MQU302tC9UiUmdGu0i9yN3LJ+g/AO5z93cBb2CUbq5mliEMI3o5sBRYaWZLh612P3Cuu58X7++2xLJvAZcdxjGMi/I1iPzwJqbBm+V0oVpE6stoCSKfmH4b8fCh7r4XGO0Rp8uATe6+2d0HgNXAiuQK7r4vHoMaoIVEzyh3fwB4ddQjGCdmRjayg2sQU8s1CN0sJyL1ZbSL1C+Z2X8FuoALgHsAzGwKkBtl23lAcji2LkLNYwgzuwr4S+AE4J2HF/aQ7a8DrgNYsGDBWDcfIpuxgy9S55pgyiw9sE9E6s5oNYgPA2cBHwKudvfdcfkbgb8fZdtqN9IddO+Eu9/p7kuAK4GbR9nnwTt0X+XuHe7e0dbWNtbNh8hF0dBHbZRNm6cahIjUndF6Me0Arq9S/nPg56PsuwuYn5hvB0Y8y7r7A2Z2mpnNcfddo+w7FdmMDX3URplGlhOROnTIBGFmaw613N3ffYjF64DFZrYI2ApcA/zesP2fDjzn7m5mFwANQM1G58lmooMftQGhJ9PWxyY+IBGRGhrtGsSbCNcRvg88zBiev+TuhfiJr/cCGeB2d99gZtfHy28F3gv8vpnlgV5CM5YDmNn3geXAHDPrAr7o7t8cy8GNVS6yg++DgHCh+sAuKPRDtjHNEEREjhmjJYgTgbcDKwnf/n8KfN/dNxzOzt39LuKeT4myWxPTtwC3jLDtysP5jPGUzUQH92KCxMhy22DmwgmNSUSkVg55kdrdi+5+j7t/kHBhehOwNu7ZNOlkMyPUIDRwkIjUodFqEJhZI6H76UpgIfA14EfphlUbh+zFBEoQIlJXRrtI/W3gbOBu4EuJu6onpar3QUDicRtKECJSP0arQXwA2A+8Dvi42eA1agPc3SfV0+uymRFqEI3TINeikeVEpK6Mdh/EaDfSTSq5aIT7IMw0spyI1J26SgCjCU1MIzxiasZ8eHXzxAYkIlJDShAJuUxUvRcTQPt/hu0bwsBBIiJ1QAkiIRsdogax4E3gJeh6ZGKDEhGpESWIhFwmqn4NAmD+Moiy8MKDExuUiEiNKEEk5EbqxQTQ0AInnacEISJ1QwkiYcT7IMpOuRC2Pgr5vokLSkSkRpQgErLRIZqYAE65CIoDIUmIiExyShAJuYyN3MQEsOANgKmZSUTqghJEwqhNTFNmwtyz4IV/m7igRERqRAkiITvSw/qSTrkQXnoEioWJCUpEpEaUIBJyIw05mnTKhZDfD6/8amKCEhGpkVQThJldZmZPm9kmM7uxyvIVZva4ma03s04zu/hwt03DiEOOJi24MLzrOoSITHKpJQgzywBfBy4HlgIrzWzpsNXuB8519/OAa4HbxrDtuCsPORqPelrd1Lkw6zR4/hdphyMiUlNp1iCWAZvcfbO7DwCrgRXJFdx9n1fOxi2AH+62achmwo/jkBeqAc68Ap69Fx669dDriYgcx9JMEPOAlxLzXXHZEGZ2lZk9RRjv+tqxbBtvf13cPNW5c+fOowo4mwnjXYx6HeKtn4cz3wX3fA7W3XZUnykicqxKM0FYlbKDzrzufqe7LwGuBG4ey7bx9qvcvcPdO9ra2o40ViAMOQqQH+06RCYH770dXnc5/PTT0Pn3R/W5IiLHojQTRBcwPzHfDow4Zqe7PwCcZmZzxrrteDnsGgRAtgF+99tw+tvhJ5+Eu/4ECv3pBigiMoHSTBDrgMVmtsjMGoBrgDXJFczsdIvHMTWzC4AGoPtwtk3D4DWI0e6FGNygEVZ+H950AzyyCm6/DHa/mGKEIiITJ7UE4e4F4AbgXmAjcIe7bzCz683s+ni19wJPmNl6Qq+lqz2oum1asZblolCDyI92kTopk4Pf+u/wu/8I3ZvgGxfC2i9rYCEROe4dckzqo+XudwF3DSu7NTF9C3DL4W6btjHXIJKWvhtOPBvu+wKs/Ut4+NZQszjnd2HGgnGOVEQkfbqTOiEXX4MYcdjR0cw6Fa7+Dly3FuZ1wL/cDF99PaxaDr/4K3jlCTjUPRYiIseQVGsQx5vc4H0QR1CDSDr5fHj/D+DVzfDkGnjyx3D/l8Jr6klw2lth4ZvDYztmnjIOkYuIjD8liIRsNIZeTIdj1qlw8SfDq+dleO5fYNPP4KmfwvrvhnWmzYMTzoTZp4fXCUtDU1XT9PGJQUTkCClBJJRrEKM+0fVITDsZzn9/eJVKsHNjeJ7Tiw/BrmfghX8PDwEsm74gXLuYOhdaTww1jdmnhSQyfQFEah0UkXQpQSQM3gcxll5MRyKKwrgSc8+CZX8UytxDLWP7Btj+a9j+JPRsha2Pwd5XoNBb2T7XXKlpTJ8fahtN02HKLGiZA60nQPOccK+GiMgRUoJIyEYp1iBGYwbT54XX635z6DJ32LcjdKPtfhZ2bAyJZMM/Qd/ukffZMBWaZ4WBjspJpGk6TJkRT8+oLJsyMySW1rnh/g6ZvNzD39vwsnIHCrODlx+tUgkKfVDKxwXx/r0Uv5zBhyUUB6B3N/S+BgP7oaEZGlrCFyOLEtsWoVQM79kmyE2B7BSIMuFlmfC3HGUOPtZiPnxOKQ9RLuz/UMfsHuIfOAD5A2HbQl/4/MbW8L+WmxJiKRbCfgv98Xrxe6kQPjeTC/FmG8P2xX4oDMDAvnC8A/vCZ2Yawsvjn12+N0xnGyHTGPYTZcOroRkWXTI+v6sEJYiE3FjupJ5IZqGpaepcWHjR0GX5Pujvgb49cOBV2L8jJJMD3WG+99Xw3t8Du7aHf7y+PUNrJMM1TodMNvwzWhT/EWbCP1LT9ErSyU2p/KE3TgtlU2ZCS1u4GD/1xPCPVz7xRJnxP/EcywZPioXKq5gPJ4/ySaQ4EMqSJ+d8b+VE0bc7nCh7d4eTSSYXft7u4cRSzIf18wfCNoW+ysmv0B+2KeVDef++sM9CXxxgnAh8hC9EUfxZyRORZSr7K/SHv49MQ/ylwhIn7fik76X4GGv4lIEoGxIHHv9M8gevYxlomhYnoUyo5Xsp/tnGP9+Rfk7Hgta58Jlnxn23ShAJ2fHqxTSRck3h1XrC2LYr9FeSRd/ukET2bQ+v/bvCP7qXKt/QSsXKN7sDr0L3c+Efp9gfktShEk5Zdkq4FjPt5JBoyidNLzF4soqyIfHkmsOJqXwiLRXjZJUJ65Y/tzgQysonMah8K7VM5cSW/FZpVvnG6qVwIk9+Gy2f4PDKt81Cb/i8Ur6SOMvfKssny+TJv9AbYhsvueZwHOUTv1n4FpltCD/XhuawTjlh55rDzzjKhWSfaYCG1vBtN9ec+Dl5/HMpfzP3yom9lA/fbMvffss/m/I34PK323JMUPkdWbzPwd9pc/i9ZnKJrt4+dL3Bv4FM5ctGriX8LAf2h9eQbeMvL2YhzkJ8Mi8VKr/LwkDld1ROZpmGEEcmF34+pXz8f7An/I7Lx2lR5W8xNyV82WloiY8j/tlbFGoVA/tCErHE32L5m3428ZlRNv77iGMaXK8h3n9rpTZTiBOrRZUakkWVv7Xy76X8N5kCJYiEci+mI74P4niSbazUSsZDYaDybXffDti7LbySz6fq2xOuq+zZCq8+H05cUZYhJ6ZSPvyTlpNPpqFygi+fzPEQf7Yp/NOVipVv52aVk52XKv9Ag0kgOVSsDT2hRVH8nqkkASzEWT4JRzkGEwtAdm4ilmx8Qs5V/qHLJ6TyMZRPSlG2cuKIcvFnxT+D5MmoaUZoEkw2+1VrIhJJgRJEQqq9mCa7bEN8DeMEaDuj1tFMbkoOMkHUVzJhTE9zFRGZ5JQgEnK17MUkInKMUYJImLD7IEREjgNKEAmVJibVIERElCASKk1MqkGIiChBJFSamFSDEBFJNUGY2WVm9rSZbTKzG6ssf5+ZPR6/HjSzcxPLPmFmT5jZBjP7ZJpxllW6uaoGISKSWoIwswxhGNHLgaXASjNbOmy154G3uPs5wM3Aqnjbs4E/ApYB5wJXmNnitGItG/fHfYuIHMfSrEEsAza5+2Z3HwBWAyuSK7j7g+7+Wjz7ENAeT58JPOTuB+Lxqf8VuCrFWAHIRBbucFcTk4hIqgliHvBSYr4rLhvJh4G74+kngEvMbLaZNQPvAOZX28jMrjOzTjPr3Llz51EFbGbkokhNTCIipPuojWrPA6h65jWzSwkJ4mIAd99oZrcA9wH7gF8BhWrbuvsq4qapjo6Ooz6zZzOmbq4iIqRbg+hi6Lf+duDl4SuZ2TnAbcAKd+8ul7v7N939Ane/BHgVeDbFWAdlI9ONciIipJsg1gGLzWyRmTUA1wBrkiuY2QLgR8AH3P2ZYctOSKzzHuD7KcY6KJeJ9KgNERFSbGJy94KZ3QDcC2SA2919g5ldHy+/FfgCMBv4hoUnVBbcvSPexQ/NbDaQBz6WuJidqmzGGCgoQYiIpPq4b3e/C7hrWNmtiek/BP5whG3fnGZsIzlldgtPbuupxUeLiBxTdCf1MJcsnsOGl3vo3lfDIRJFRI4BShDDXLy4DYBfbtpV40hERGpLCWKY18+bzvQpOX7xrBKEiNQ3JYhhMpFx8elz+OWzu3BXd1cRqV9KEFVcvHgOr/T0sWnHvlqHIiJSM0oQVVx8+hwAHlAzk4jUMSWIKubPaubUOS388tmje7aTiMjxTAliBG9ePIeHNr9Kf6FY61BERGpCCWIEFy9uozdf5NEXJuQGbhGRY44SxAjeeOosspGpu6uI1C0liBFMbcrxxlNnc+djW/VsJhGpS0oQh/BHl5zKKz193PkfXbUORURkwilBHMIli+fw+nnT+du1z2kQIRGpO0oQh2BmfOzS09jSfYC7nnil1uGIiEwoJYhR/ObSEzn9hFa+8fNNevSGiNQVJYhRRJHx0eWn8dQre7l/445ahyMiMmFSTRBmdpmZPW1mm8zsxirL32dmj8evB83s3MSyPzazDWb2hJl938ya0oz1UN517sm0z5zC//zZM7oWISJ1I7UEYWYZ4OvA5cBSYKWZLR222vPAW9z9HOBmYFW87Tzg40CHu59NGLL0mrRiHU0uE3Hj5UvY8HIP33pwS63CEBGZUGnWIJYBm9x9s7sPAKuBFckV3P3BxFjTDwHticVZYIqZZYFm4OUUYx3VO19/Epee0cZf3fcMW3f31jIUEZEJkWaCmAe8lJjvistG8mHgbgB33wp8BXgR2Abscfd/rraRmV1nZp1m1rlzZ3oP1zMz/nzF2bjDF/7pCV2wFpFJL80EYVXKqp5VzexSQoL4XDw/k1DbWAScDLSY2furbevuq9y9w9072traxiXwkcyf1cynf/N13P/UDu5Wt1cRmeTSTBBdwPzEfDtVmonM7BzgNmCFu3fHxb8BPO/uO909D/wIuDDFWA/bhy5cyNnzpnHTj37Nxm09tQ5HRCQ1aSaIdcBiM1tkZg2Ei8xrkiuY2QLCyf8D7v5MYtGLwBvNrNnMDHgbsDHFWA9bNhPxt+/7TzQ3ZHj/bQ+zacfeWockIpKK1BKEuxeAG4B7CSf3O9x9g5ldb2bXx6t9AZgNfMPM1ptZZ7ztw8APgMeAX8dxrkor1rGaP6uZ7/7hGzAzfu/vHmbLrv21DklEZNzZZLrY2tHR4Z2dnRP2eU+/spdrVv07mSjiL648i8vOPmnCPltEZDyY2aPu3lFtme6kPgpnnDiV1de9iROmNnL9dx7jun/o5JU9fbUOS0RkXChBHKUzTpzKj2+4iBsvX8K/PrOTS7+yli/++Ale7D5Q69BERI6KmpjG0Qvd+/mbf9nEj9dvpVhy3r50Lu8+dx6XLmmjuSFbs7hEREZyqCYmJYgUbO/p49sPbuGOzpfYtW+AplzEJYvbePPiObzptDmc1tZC6JwlIlJbShA1Uiw567a8yt2/3sZ9T27n5fj6RNvURs6bP4Nz26dzTvsMlp48jTmtjTWOVkTqkRLEMcDdefHVAzz4XDcPb+7m8a49bE50jz1haiNnnjSN181tZfEJUzl9biunzWllenOuhlGLyGR3qAShhvEJYmacMruFU2a3sHLZAgD29ObZsHUPT27rYeO2vWzc1sNDm7vpL1QeKT67pYFFc1pYOKeFRXNaOGV2M6fMamHBrGYlDxFJlWoQx5hiyel67QDPbt/H5l37eH7Xfp7buZ8tu/azY2//kHWnNmWZP7OZeTOnMG9GeJ00o4mTpjcxd1oTbVMbacxmanQkInI8UA3iOJKJKjUNmDtk2YGBAlt2HeCl1w7w0qsHeKH7AFt39/JC937+bdMuDgwUD9rf9Ck52qY20tbaSNvURua0NjJnagNzWhqZ3drArJYGZrc0MrMlR2tjVhfPRWSQEsRxpLkhy9KTp7H05GkHLXN3enoLbOvpZdvuPnbs7WNHTz879/Wzo6efXfv6ebxrNzv39rO/SiIByEbGjOYcM5obmDElvM9szjF9So5pU3JMa8oyvTnHtKZQNrUpx7QpWaY25WhpyCi5iEwyShCThJkxvTnH9OYcS048OIEk9Q4U2bUvJI3XDgzw6v48r+7vZ/eBPK8dyLP7wAC7D+Tpeu0AT2zN09OXr1o7SYoMWhtDspjalGVqU5bWxiytTaFmMi2eb2msvLc0ZpjaFKabc1maGzO0NGRpykVKNiLHACWIOjSlIcP8Wc3Mn9V82NsMFEr09OXZ21dgT2+ePb159sbzlfcCPX159vUV2NdfYNe+AbZ0H2BvX56evgIDhcMbz9sMpuQyNDeEJBKmMzTlMjRmI6Y0xMsaMjQ3ZmnKZmjKRTTlwrqNienKe0RjNl4WvzdmlYhEDkUJQg5LQzYK1y+O4n6NgUKJAwMheezvL7Kvvzxd4MBAkQMDobx3oMD+eD6Uh+n+fIm9fQV680V6B4rsHwjb5otH1tGinIgGE0s2oiF+NWbjhJKYb0iUNeYiGjKVRNOQjWjIVNbNZSqvyv4q82GZkctEZDNGLoqIIiUrObYoQciECSffBmY0N4zrfoslpy9fpDdfpC9fpC9foi9fpL9QpHegRG883Z8v0Re/V9YtxgmnRH+hyEChRH+hxEAhrLO7d4D+fImBYijry1fWKZTGtwdgJrKQNKKIXDYiG1USSCYyGjIHJ5ZsFOaz8TaV7UNZNjIyGSNjNrhtNhOXRxa/x0kqk9gm/uxMZPGyKLF+vF78WdmMDcaSsfB52cT2cvxSgpDjXiay+JrGxP45F4ohcfTnQ8LIFyuJJF8Mr3KyKSeVcnlYxymUwnu5fMh0wSmUwjqFRPlAMcz35UsUSkXyhdLgOv3xdHk/pVJ5H05xnBPa4TCDXBQSRWZIUrLB5FVeHpkRRUYmYjBRDSae5HaZOKkl9hNFRmSEBBVVkmrGQnkU779aDNlhya+8XbXPKr8ii/cdEeJOfE5526gcm1ViLM+bMbhNJrJjtqlTCULkCGUzEdlMxDhXiFLj7oNJqRgnjHyx/F6Kk0h4LyekQqmcaBhMQkUPy8uJqFAskS85xWKJojOYlMqJsFBySvE2xVKJoh/82cV4nWKJsG4p7LdQdHqLxcH5ctzFRNLLF0uUnHh7TyTFUF6LxDhWNpjcKgnIyuVxLS+XGZrkDMDAgNktjdxx/ZvGPa5UE4SZXQb8NZABbnP3Lw9b/j7gc/HsPuAj7v4rMzsD+D+JVU8FvuDuX00zXpHJzMxoyBoNdfiU/1I5AbkPSTLlJFVOmvmiDyaaZCIqFCuJrVByPE5mIfn4YCJKblsaXKc0JKE5IZmFmIZt506xGModx+PEV07ExcR2JQ/7wmHalHRO5aklCDPLAF8H3g50AevMbI27P5lY7XngLe7+mpldThhW9A3u/jRwXmI/W4E704pVRCa3KDIiTE0mY5TmV4llwCZ33+zuA8BqYEVyBXd/0N1fi2cfAtqr7OdtwHPu/kKKsYqIyDBpJoh5wEuJ+a64bCQfBu6uUn4N8P2RNjKz68ys08w6d+7ceUSBiojIwdJMENUuy1e9WmRmlxISxOeGlTcA7wb+70gf4u6r3L3D3Tva2tqOIlwREUlKs0muC5ifmG8HXh6+kpmdA9wGXO7u3cMWXw485u7bU4tSRESqSrMGsQ5YbGaL4prANcCa5ApmtgD4EfABd3+myj5WcojmJRERSU9qNQh3L5jZDcC9hG6ut7v7BjO7Pl5+K/AFYDbwjfhGkUL5ueRm1kzoAfVf0opRRERGpgGDRETq2KEGDKq/O2ZEROSwTKoahJntBI70fok5wK5xDOd4UI/HDPV53PV4zFCfxz3WYz7F3at2AZ1UCeJomFnnSNWsyaoejxnq87jr8ZihPo97PI9ZTUwiIlKVEoSIiFSlBFGxqtYB1EA9HjPU53HX4zFDfR73uB2zrkGIiEhVqkGIiEhVShAiIlJV3ScIM7vMzJ42s01mdmOt40mLmc03s5+b2UYz22Bmn4jLZ5nZfWb2bPw+s9axjjczy5jZf5jZT+L5ejjmGWb2AzN7Kv6dv2myH7eZ/XH8t/2EmX3fzJom4zGb2e1mtsPMnkiUjXicZnZTfH572sx+ayyfVdcJIjHq3eXAUmClmS2tbVSpKQCfdvczgTcCH4uP9UbgfndfDNwfz082nwA2Jubr4Zj/GrjH3ZcA5xKOf9Iet5nNAz4OdLj72YTnv13D5DzmbwGXDSurepzx//g1wFnxNt+Iz3uHpa4TBIcx6t1k4e7b3P2xeHov4YQxj3C8345X+zZwZU0CTImZtQPvJDxSvmyyH/M04BLgmwDuPuDuu5nkx014+OgUM8sCzYThBSbdMbv7A8Crw4pHOs4VwGp373f354FNhPPeYan3BDHWUe8mBTNbCJwPPAzMdfdtEJIIcEINQ0vDV4HPAqVE2WQ/5lOBncDfx01rt5lZC5P4uN19K/AV4EVgG7DH3f+ZSXzMw4x0nEd1jqv3BHHYo95NFmbWCvwQ+KS799Q6njSZ2RXADnd/tNaxTLAscAHwt+5+PrCfydG0MqK4zX0FsAg4GWgxs/fXNqpjwlGd4+o9QRzWqHeThZnlCMnhu+7+o7h4u5mdFC8/CdhRq/hScBHwbjPbQmg+fKuZfYfJfcwQ/q673P3heP4HhIQxmY/7N4Dn3X2nu+cJA5FdyOQ+5qSRjvOoznH1niBGHfVusrAwItM3gY3u/leJRWuAD8bTHwR+PNGxpcXdb3L3dndfSPjd/ou7v59JfMwA7v4K8JKZnREXvQ14ksl93C8CbzSz5vhv/W2E62yT+ZiTRjrONcA1ZtZoZouAxcAjh71Xd6/rF/AO4BngOeC/1TqeFI/zYkLV8nFgffx6B2FEv/uBZ+P3WbWONaXjXw78JJ6e9McMnAd0xr/vfwJmTvbjBr4EPAU8Afwj0DgZj5kwDPM2IE+oIXz4UMcJ/Lf4/PY0cPlYPkuP2hARkarqvYlJRERGoAQhIiJVKUGIiEhVShAiIlKVEoSIiFSlBCEyBmZWNLP1ide43aFsZguTT+gUqbVsrQMQOc70uvt5tQ5CZCKoBiEyDsxsi5ndYmaPxK/T4/JTzOx+M3s8fl8Ql881szvN7Ffx68J4Vxkz+7t4XIN/NrMpNTsoqXtKECJjM2VYE9PViWU97r4M+F+Ep8gST/+Du58DfBf4Wlz+NeBf3f1cwnOSNsTli4Gvu/tZwG7gvakejcgh6E5qkTEws33u3lqlfAvwVnffHD8U8RV3n21mu4CT3D0fl29z9zlmthNod/f+xD4WAvd5GPQFM/sckHP3v5iAQxM5iGoQIuPHR5geaZ1q+hPTRXSdUGpICUJk/FydeP/3ePpBwpNkAd4H/DKevh/4CAyOmT1tooIUOVz6diIyNlPMbH1i/h53L3d1bTSzhwlfvFbGZR8HbjezPyGM8vYHcfkngFVm9mFCTeEjhCd0ihwzdA1CZBzE1yA63H1XrWMRGS9qYhIRkapUgxARkapUgxARkaqUIEREpColCBERqUoJQkREqlKCEBGRqv4/1/W2SYv3YnMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 99 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 98 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 97 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 96 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 95 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 94 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 93 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 92 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 91 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 90 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 89 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 88 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 87 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 86 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 85 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 84 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 83 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 82 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 81 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 80 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 79 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 78 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 77 \n",
      "\tTrain Loss: 0.271 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 76 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 75 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 74 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 73 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 72 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 71 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 70 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 69 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 68 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 67 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 66 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 65 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 64 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 63 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 62 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 61 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 60 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 59 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 58 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 57 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 56 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 55 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 54 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 53 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 52 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 51 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 50 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 49 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 48 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 47 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 46 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 45 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 44 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 43 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 42 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 41 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 40 \n",
      "\tTrain Loss: 0.272 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 39 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 38 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 37 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 36 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 35 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 34 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 33 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 32 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 31 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 30 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 29 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.294 \n",
      "\n",
      "Epoch: 28 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 27 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 26 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 25 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 24 \n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 23 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 22 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 21 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 20 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 19 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 18 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 17 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 16 \n",
      "\tTrain Loss: 0.274 \n",
      "\t Val. Loss: 0.295 \n",
      "\n",
      "Epoch: 15 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.296 \n",
      "\n",
      "Epoch: 14 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.296 \n",
      "\n",
      "Epoch: 13 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.296 \n",
      "\n",
      "Epoch: 12 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.296 \n",
      "\n",
      "Epoch: 11 \n",
      "\tTrain Loss: 0.275 \n",
      "\t Val. Loss: 0.296 \n",
      "\n",
      "Epoch: 10 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.297 \n",
      "\n",
      "Epoch: 09 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.297 \n",
      "\n",
      "Epoch: 08 \n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 0.297 \n",
      "\n",
      "Epoch: 07 \n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 0.298 \n",
      "\n",
      "Epoch: 06 \n",
      "\tTrain Loss: 0.278 \n",
      "\t Val. Loss: 0.299 \n",
      "\n",
      "Epoch: 05 \n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 0.300 \n",
      "\n",
      "Epoch: 04 \n",
      "\tTrain Loss: 0.281 \n",
      "\t Val. Loss: 0.305 \n",
      "\n",
      "Epoch: 03 \n",
      "\tTrain Loss: 0.283 \n",
      "\t Val. Loss: 0.317 \n",
      "\n",
      "Epoch: 02 \n",
      "\tTrain Loss: 0.288 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "Epoch: 01 \n",
      "\tTrain Loss: 0.350 \n",
      "\t Val. Loss: 0.327 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2942940705531352"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "fit(model,\n",
    "    optimiser=optimizer,\n",
    "    criterion=criterion,\n",
    "    res_train_test_split=res_train_test_split,\n",
    "    n_epochs=100,\n",
    "    batch_size=128,\n",
    "    best_model_file='./best_ae.pth',\n",
    "    points_ahead=1,\n",
    "    random_state=777,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "58cc8a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=0.41\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_pred_test =  model.run_epoch(Loader(res_train_test_split[1],res_train_test_split[0],32), optimizer=None, criterion=None, phase='forecast').squeeze(1)\n",
    "RMSE = mse(y_pred_test,np.squeeze(res_train_test_split[-1],1),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0818a4",
   "metadata": {},
   "source": [
    "# LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53fc3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, in_features, n_hidden, n_layers=3, bidirectional=False, dropout=0.5, seed=None):\n",
    "        set_determenistic(seed)\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size=in_features,\n",
    "                            hidden_size=n_hidden,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=dropout,\n",
    "                            batch_first =True,\n",
    "                            bidirectional = bidirectional\n",
    "                           )\n",
    "        self.k_bidir = 2 if bidirectional else 1 \n",
    "\n",
    "        self.linear = nn.Linear(in_features=n_hidden, out_features=n_hidden)        \n",
    "    \n",
    "    \n",
    "    def initHidden(self,batch_size,device):\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.n_layers*self.k_bidir, batch_size, self.n_hidden).to(device),\n",
    "            torch.zeros(self.n_layers*self.k_bidir, batch_size, self.n_hidden).to(device)\n",
    "        )\n",
    "    def forward(self, sequences):\n",
    "        batch_size  = len(sequences)\n",
    "        lstm_out, self.hidden = self.lstm(sequences, self.hidden )\n",
    "        last_time_step = lstm_out.reshape(-1, batch_size, self.n_hidden)[-1] # -1 is len_seq\n",
    "\n",
    "        y_pred = self.linear(last_time_step)\n",
    "        return y_pred\n",
    "\n",
    "    def run_epoch(self, iterator, optimizer, criterion, points_ahead=1, phase='train', device=torch.device('cuda:0'), encod_decode_model=False):\n",
    "        self.to(device)\n",
    "        \n",
    "        is_train = (phase == 'train')\n",
    "        if is_train:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        if points_ahead !=1:\n",
    "            assert (points_ahead > 0) & (type(points_ahead)==type(int()))\n",
    "            def forecast_multistep(y_pred,points_ahead):\n",
    "                new_x = y_pred\n",
    "                for j in range(points_ahead-1):\n",
    "                    new_x = self.forward(new_x).unsqueeze(1)\n",
    "                    y_pred = torch.cat([y_pred,new_x],1)\n",
    "                return y_pred\n",
    "        else:\n",
    "            def forecast_multistep(y_pred,points_ahead):\n",
    "                return y_pred\n",
    "\n",
    "        all_y_preds = []\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            for i, (x,y) in enumerate(iterator):\n",
    "                x,y = np.array(x),np.array(y) #df.index rif of\n",
    "                self.initHidden(x.shape[0],device=device)\n",
    "                \n",
    "                x = torch.tensor(x).float().to(device).requires_grad_()\n",
    "                y_true = torch.tensor(y).float().to(device)\n",
    "                y_pred = self.forward(x).unsqueeze(1)\n",
    "                y_pred = forecast_multistep(y_pred,points_ahead)\n",
    "                \n",
    "                if encod_decode_model:\n",
    "                    y_pred = torch.cat([y_pred[:,i,:].unsqueeze(1) for i in range(y_pred.shape[1]-1,-1,-1)],1)\n",
    "                \n",
    "                if phase == 'forecast':\n",
    "                    all_y_preds.append(y_pred)\n",
    "                    continue # in case of pahse = 'forecast' criterion is None\n",
    "                        \n",
    "                loss = criterion(y_pred,y_true)\n",
    "                if is_train:\n",
    "                  optimizer.zero_grad()\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "        if phase != 'forecast':\n",
    "            return epoch_loss / len(iterator)#, n_true_predicted / n_predicted\n",
    "        else:\n",
    "            return torch.cat(all_y_preds).detach().cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1264a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_test_split = (X_train_ts, X_test_ts,  y_train_ts, y_test_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b98f02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2952\n"
     ]
    }
   ],
   "source": [
    "model = SimpleLSTM(res_train_test_split[0][0].shape[1],\n",
    "                   res_train_test_split[-1][0].shape[1],\n",
    "                   n_layers=5,\n",
    "                   dropout=0.0,\n",
    "                   seed=26)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Net paramaeters: {pytorch_total_params}')\n",
    "print(f'Samples: {len(res_train_test_split[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31aeabd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 \n",
      "\tTrain Loss: 0.975 \n",
      "\t Val. Loss: 1.199 \n",
      "\n",
      "Epoch: 99 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.200 \n",
      "\n",
      "Epoch: 98 \n",
      "\tTrain Loss: 0.975 \n",
      "\t Val. Loss: 1.196 \n",
      "\n",
      "Epoch: 97 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 96 \n",
      "\tTrain Loss: 0.975 \n",
      "\t Val. Loss: 1.193 \n",
      "\n",
      "Epoch: 95 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.190 \n",
      "\n",
      "Epoch: 94 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 93 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.188 \n",
      "\n",
      "Epoch: 92 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.189 \n",
      "\n",
      "Epoch: 91 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.168 \n",
      "\n",
      "Epoch: 90 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 89 \n",
      "\tTrain Loss: 0.975 \n",
      "\t Val. Loss: 1.176 \n",
      "\n",
      "Epoch: 88 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.164 \n",
      "\n",
      "Epoch: 87 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.182 \n",
      "\n",
      "Epoch: 86 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 85 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.174 \n",
      "\n",
      "Epoch: 84 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.188 \n",
      "\n",
      "Epoch: 83 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.177 \n",
      "\n",
      "Epoch: 82 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.178 \n",
      "\n",
      "Epoch: 81 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.179 \n",
      "\n",
      "Epoch: 80 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.181 \n",
      "\n",
      "Epoch: 79 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.179 \n",
      "\n",
      "Epoch: 78 \n",
      "\tTrain Loss: 0.976 \n",
      "\t Val. Loss: 1.173 \n",
      "\n",
      "Epoch: 77 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 76 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.178 \n",
      "\n",
      "Epoch: 75 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.178 \n",
      "\n",
      "Epoch: 74 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.187 \n",
      "\n",
      "Epoch: 73 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.175 \n",
      "\n",
      "Epoch: 72 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.163 \n",
      "\n",
      "Epoch: 71 \n",
      "\tTrain Loss: 0.977 \n",
      "\t Val. Loss: 1.176 \n",
      "\n",
      "Epoch: 70 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.173 \n",
      "\n",
      "Epoch: 69 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 68 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.191 \n",
      "\n",
      "Epoch: 67 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.171 \n",
      "\n",
      "Epoch: 66 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.173 \n",
      "\n",
      "Epoch: 65 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.180 \n",
      "\n",
      "Epoch: 64 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.162 \n",
      "\n",
      "Epoch: 63 \n",
      "\tTrain Loss: 0.978 \n",
      "\t Val. Loss: 1.177 \n",
      "\n",
      "Epoch: 62 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.171 \n",
      "\n",
      "Epoch: 61 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.172 \n",
      "\n",
      "Epoch: 60 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.169 \n",
      "\n",
      "Epoch: 59 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.172 \n",
      "\n",
      "Epoch: 58 \n",
      "\tTrain Loss: 0.979 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 57 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.180 \n",
      "\n",
      "Epoch: 56 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 55 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.195 \n",
      "\n",
      "Epoch: 54 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.178 \n",
      "\n",
      "Epoch: 53 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.186 \n",
      "\n",
      "Epoch: 52 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.181 \n",
      "\n",
      "Epoch: 51 \n",
      "\tTrain Loss: 0.980 \n",
      "\t Val. Loss: 1.179 \n",
      "\n",
      "Epoch: 50 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 49 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 48 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.181 \n",
      "\n",
      "Epoch: 47 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.190 \n",
      "\n",
      "Epoch: 46 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.180 \n",
      "\n",
      "Epoch: 45 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.180 \n",
      "\n",
      "Epoch: 44 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 43 \n",
      "\tTrain Loss: 0.981 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 42 \n",
      "\tTrain Loss: 0.982 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 41 \n",
      "\tTrain Loss: 0.982 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 40 \n",
      "\tTrain Loss: 0.982 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 39 \n",
      "\tTrain Loss: 0.982 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 38 \n",
      "\tTrain Loss: 0.982 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 37 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 36 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 35 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 34 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 33 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 32 \n",
      "\tTrain Loss: 0.983 \n",
      "\t Val. Loss: 1.179 \n",
      "\n",
      "Epoch: 31 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.177 \n",
      "\n",
      "Epoch: 30 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.175 \n",
      "\n",
      "Epoch: 29 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.180 \n",
      "\n",
      "Epoch: 28 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.181 \n",
      "\n",
      "Epoch: 27 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 26 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.183 \n",
      "\n",
      "Epoch: 25 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 24 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 23 \n",
      "\tTrain Loss: 0.984 \n",
      "\t Val. Loss: 1.189 \n",
      "\n",
      "Epoch: 22 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.186 \n",
      "\n",
      "Epoch: 21 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 20 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.172 \n",
      "\n",
      "Epoch: 19 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.161 \n",
      "\n",
      "Epoch: 18 \n",
      "\tTrain Loss: 0.985 \n",
      "\t Val. Loss: 1.176 \n",
      "\n",
      "Epoch: 17 \n",
      "\tTrain Loss: 0.986 \n",
      "\t Val. Loss: 1.170 \n",
      "\n",
      "Epoch: 16 \n",
      "\tTrain Loss: 0.986 \n",
      "\t Val. Loss: 1.179 \n",
      "\n",
      "Epoch: 15 \n",
      "\tTrain Loss: 0.986 \n",
      "\t Val. Loss: 1.169 \n",
      "\n",
      "Epoch: 14 \n",
      "\tTrain Loss: 0.987 \n",
      "\t Val. Loss: 1.175 \n",
      "\n",
      "Epoch: 13 \n",
      "\tTrain Loss: 0.987 \n",
      "\t Val. Loss: 1.181 \n",
      "\n",
      "Epoch: 12 \n",
      "\tTrain Loss: 0.987 \n",
      "\t Val. Loss: 1.185 \n",
      "\n",
      "Epoch: 11 \n",
      "\tTrain Loss: 0.987 \n",
      "\t Val. Loss: 1.188 \n",
      "\n",
      "Epoch: 10 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.188 \n",
      "\n",
      "Epoch: 09 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.187 \n",
      "\n",
      "Epoch: 08 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.184 \n",
      "\n",
      "Epoch: 07 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.176 \n",
      "\n",
      "Epoch: 06 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.167 \n",
      "\n",
      "Epoch: 05 \n",
      "\tTrain Loss: 0.988 \n",
      "\t Val. Loss: 1.166 \n",
      "\n",
      "Epoch: 04 \n",
      "\tTrain Loss: 0.989 \n",
      "\t Val. Loss: 1.138 \n",
      "\n",
      "Epoch: 03 \n",
      "\tTrain Loss: 0.989 \n",
      "\t Val. Loss: 1.129 \n",
      "\n",
      "Epoch: 02 \n",
      "\tTrain Loss: 0.990 \n",
      "\t Val. Loss: 1.134 \n",
      "\n",
      "Epoch: 01 \n",
      "\tTrain Loss: 0.990 \n",
      "\t Val. Loss: 1.144 \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1294832091282658"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "fit(model,\n",
    "    optimiser=optimizer,\n",
    "    criterion=criterion,\n",
    "    res_train_test_split=res_train_test_split,\n",
    "    n_epochs=100,\n",
    "    batch_size=32,\n",
    "    best_model_file='./best_ae.pth',\n",
    "    points_ahead=1,\n",
    "    random_state=777,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa4a606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.04\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_pred_test =  model.run_epoch(Loader(res_train_test_split[1],res_train_test_split[0],32), optimizer=None, criterion=None, phase='forecast').squeeze(1)\n",
    "RMSE = mse(y_pred_test,np.squeeze(res_train_test_split[-1],1),squared=False)\n",
    "print(f'RMSE={RMSE:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c0747",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06857826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        print(input)\n",
    "        print(f'Input shape {input.shape}')\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        print(f'embedded shape {embedded.shape}')\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        print(f'attn_weights shape {attn_weights.shape}')\n",
    "        print(f'encoder_outputs shape {encoder_outputs.shape}')\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        print(f'attn_applied shape {attn_applied.shape}')\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        print(f'output shape {output.shape}')\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        print(f'output shape {output.shape}')\n",
    "        print(f'hidden shape {hidden.shape}')\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    def run_epoch(self, iterator, optimizer, criterion, points_ahead=1, phase='train', device=torch.device('cuda:0'), encod_decode_model=False):\n",
    "    self.to(device)\n",
    "\n",
    "    is_train = (phase == 'train')\n",
    "    if is_train:\n",
    "        self.train()\n",
    "    else:\n",
    "        self.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if points_ahead !=1:\n",
    "        assert (points_ahead > 0) & (type(points_ahead)==type(int()))\n",
    "        def forecast_multistep(y_pred,points_ahead):\n",
    "            new_x = y_pred\n",
    "            for j in range(points_ahead-1):\n",
    "                new_x = self.forward(new_x).unsqueeze(1)\n",
    "                y_pred = torch.cat([y_pred,new_x],1)\n",
    "            return y_pred\n",
    "    else:\n",
    "        def forecast_multistep(y_pred,points_ahead):\n",
    "            return y_pred\n",
    "\n",
    "    all_y_preds = []\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for i, (x,y) in enumerate(iterator):\n",
    "            x,y = np.array(x),np.array(y) #df.index rif of\n",
    "            self.initHidden(x.shape[0],device=device)\n",
    "\n",
    "            x = torch.tensor(x).float().to(device).requires_grad_()\n",
    "            y_true = torch.tensor(y).float().to(device)\n",
    "            y_pred = self.forward(x).unsqueeze(1)\n",
    "            y_pred = forecast_multistep(y_pred,points_ahead)\n",
    "\n",
    "            if encod_decode_model:\n",
    "                y_pred = torch.cat([y_pred[:,i,:].unsqueeze(1) for i in range(y_pred.shape[1]-1,-1,-1)],1)\n",
    "\n",
    "            if phase == 'forecast':\n",
    "                all_y_preds.append(y_pred)\n",
    "                continue # in case of pahse = 'forecast' criterion is None\n",
    "\n",
    "            loss = criterion(y_pred,y_true)\n",
    "            if is_train:\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    if phase != 'forecast':\n",
    "        return epoch_loss / len(iterator)#, n_true_predicted / n_predicted\n",
    "    else:\n",
    "        return torch.cat(all_y_preds).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82842736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7e63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    print(f'attn_applied shape {max_length}')\n",
    "    print(f'input_length shape {input_length}')\n",
    "    print(f'target_length shape {target_length}')\n",
    "    \n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
